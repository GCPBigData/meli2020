{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import joblib\n",
    "\n",
    "import json\n",
    "import tqdm\n",
    "\n",
    "import numba\n",
    "import dask\n",
    "import xgboost\n",
    "from dask.diagnostics import ProgressBar\n",
    "ProgressBar().register()\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "fold1, fold2 = joblib.load(\"./valid/fold1.pkl.z\"), joblib.load(\"./valid/fold2.pkl.z\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet(\"./data/train.parquet\")\n",
    "train_melt = pd.read_parquet(\"./data/22c_train_melt_with_features.parquet\")\n",
    "test_melt = pd.read_parquet(\"./data/22c_test_melt_with_features.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_data = pd.read_parquet(\"./data/item_data.parquet\")\n",
    "\n",
    "item_title_map = item_data[['item_id', 'title']].drop_duplicates()\n",
    "item_title_map = item_title_map.set_index(\"item_id\").squeeze().to_dict()\n",
    "\n",
    "item_price_map = item_data[['item_id', 'price']].drop_duplicates()\n",
    "item_price_map = item_price_map.set_index(\"item_id\").squeeze().to_dict()\n",
    "\n",
    "item_domain_map = item_data[['item_id', 'domain_id']].drop_duplicates()\n",
    "item_domain_map = item_domain_map.set_index(\"item_id\").squeeze().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_np = joblib.load(\"22a_embs_np.pkl.z\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_emb_map = {t: embs_np[i] for i, t in enumerate(item_data['item_id'].values)} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bags = train_melt[train_melt['viewed'] == 1].groupby(\"seq_index\", dropna=False)['event_info'].apply(lambda x: x.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bags_mean = np.zeros((train_melt['seq_index'].max()+1, 1024), dtype=np.float32)\n",
    "for ilist in tqdm.tqdm(bags.index):\n",
    "    if ilist in bags:\n",
    "        bags_mean[ilist, :] = np.mean([item_emb_map[e] for e in bags[ilist]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BagOfEmbeds(torch.utils.data.Dataset):\n",
    "    def __init__(self, bags, max_seq=3):\n",
    "        super().__init__()\n",
    "        self.max_seq = max_seq\n",
    "        self.bags = bags\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.bags)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if index in self.bags:\n",
    "            return torch.Tensor(embs_np[self.bags[index][:self.max_seq]])\n",
    "        return torch.zeros((1, 1024))\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, seq_ixs, candidates, nums, y):\n",
    "        super().__init__()\n",
    "        self.seq_ixs = seq_ixs.values\n",
    "        self.candidates = candidates.values\n",
    "        self.nums = nums\n",
    "        self.y = y.values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.seq_ixs.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        #print(self.seq_ixs[index].item())\n",
    "        x = bags_mean[self.seq_ixs[index]]\n",
    "        c = item_emb_map[self.candidates[index]]\n",
    "        n = self.nums[index]\n",
    "        y = self.y[index].astype(np.float32)#.reshape(-1,1)\n",
    "        \n",
    "        return x, c, n, y\n",
    "\n",
    "log_pos = np.log1p(np.arange(1,11))\n",
    "best_sellers = [1587422, 1803710,   10243,  548905, 1906937,  716822, 1361154, 1716388,  725371,  859574]\n",
    "best_sellers_domain = [item_domain_map[e] for e in best_sellers]\n",
    "\n",
    "def pad(lst):\n",
    "    \n",
    "    if len(lst) == 0:\n",
    "        return best_sellers\n",
    "    if len(lst) < 10:\n",
    "        lst += best_sellers[:(10 - len(lst))]\n",
    "    return np.array(lst)\n",
    "\n",
    "def pad_str(lst):\n",
    "    if len(lst) == 0:\n",
    "        return best_sellers_domain\n",
    "    if len(lst) < 10:\n",
    "        lst += best_sellers_domain[:(10 - len(lst))]\n",
    "    return lst\n",
    "\n",
    "def ndcg_vec(ytrue, ypred, ytrue_domain, ypred_domain):\n",
    "    relevance = np.zeros((ypred.shape[0], 10))\n",
    "    for i in range(10):\n",
    "        relevance[:, i] = np.equal(ypred_domain[:, i], ytrue_domain) * (np.equal(ypred[:, i], ytrue) * 12 + 1)\n",
    "    dcg = (relevance / log_pos).sum(axis=1)\n",
    "\n",
    "    i_relevance = np.ones(10)\n",
    "    i_relevance[0] = 12.\n",
    "    idcg = np.zeros(ypred.shape[0]) + (i_relevance / log_pos).sum()\n",
    "\n",
    "    return (dcg / idcg).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['item_price', 'seq_pos', 'n_views',\n",
    "           'n_views_this', 'n_views_this_domain', 'unique_items_viewed',\n",
    "           'unique_domains_viewed', 'n_searches', 'n_unique_searches',\n",
    "           'avg_search_seqpos', 'avg_search_len', 'avg_search_words', 'n_views_this_ratio', 'n_views_this_ratio_domain', 'viewed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hid1_x = nn.Linear(1024, 512)\n",
    "        self.hid1_c = nn.Linear(1024, 512)\n",
    "        self.hid1_n = nn.Linear(len(features), 512)\n",
    "        \n",
    "        self.hid1 = nn.Linear(512*3, 512)\n",
    "        self.out = nn.Linear(512, 1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, c, n):\n",
    "        ox = F.relu(self.hid1_x(x))\n",
    "        \n",
    "        oc = F.relu(self.hid1_c(c))\n",
    "        \n",
    "        on = F.relu(self.hid1_n(n))\n",
    "        \n",
    "        o = torch.cat([ox,oc, on], dim=-1)\n",
    "        o = F.relu(self.hid1(o))\n",
    "        o = self.out(o)\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_p = list()\n",
    "for f1, f2 in [(fold1, fold2), (fold2, fold1)]:\n",
    "    Xtr = train_melt[train_melt['seq_index'].isin(f1)]\n",
    "    Xval = train_melt[train_melt['seq_index'].isin(f2)]\n",
    "\n",
    "    #len(fold2), len(fold1)\n",
    "\n",
    "    #Xtr.shape, Xval.shape\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    Xtr_nums = scaler.fit_transform(Xtr[features]).astype(np.float32)\n",
    "    Xtr_nums[np.isnan(Xtr_nums)] = 0\n",
    "    Xval_nums = scaler.transform(Xval[features]).astype(np.float32)\n",
    "    Xval_nums[np.isnan(Xval_nums)] = 0\n",
    "\n",
    "    torch.manual_seed(0)\n",
    "    mdl = NN().cuda()\n",
    "    optimizer = Adam(mdl.parameters(), lr=1e-5)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    #del tl, vl\n",
    "    t = Dataset(Xtr['seq_index'], Xtr['event_info'], Xtr_nums, Xtr['y_rank'])\n",
    "    tl = torch.utils.data.DataLoader(t, batch_size=128, num_workers=8, pin_memory=True, shuffle=True)\n",
    "\n",
    "    v = Dataset(Xval['seq_index'], Xval['event_info'], Xval_nums, Xval['y_rank'])\n",
    "    vl = torch.utils.data.DataLoader(v, batch_size=1024, num_workers=4, pin_memory=True, shuffle=False)\n",
    "\n",
    "    for epoch in range(5):\n",
    "        tr_loss = 0\n",
    "        tr_cnt = 0\n",
    "        #tl = torch.utils.data.DataLoader(t, batch_size=128, num_workers=8, pin_memory=True, shuffle=True)\n",
    "        for x, c, n, y in tqdm.tqdm(tl):\n",
    "            #print(x.shape, c.shape, y.shape)\n",
    "            x = x.cuda()\n",
    "            c = c.cuda()\n",
    "            n = n.cuda()\n",
    "            y = y.cuda().unsqueeze(-1)\n",
    "\n",
    "            mdl.zero_grad()\n",
    "\n",
    "            p = mdl(x,c,n)\n",
    "\n",
    "            loss = criterion(p, y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            tr_cnt += 1\n",
    "            #if tr_cnt % 1000 == 0:\n",
    "            #    print(tr_cnt, tr_loss/tr_cnt)\n",
    "            #    break\n",
    "        print(epoch, tr_loss/tr_cnt)\n",
    "\n",
    "\n",
    "\n",
    "    p = list()\n",
    "    with torch.no_grad():\n",
    "        for x, c, n, y in tqdm.tqdm(vl):\n",
    "            #print(x.shape, c.shape, y.shape)\n",
    "            x = x.cuda()\n",
    "            c = c.cuda()\n",
    "            n = n.cuda()\n",
    "            y = y.cuda()\n",
    "            p.append(mdl(x,c,n))\n",
    "\n",
    "        p = torch.cat(p)\n",
    "\n",
    "    preds = Xval[['seq_index', 'has_bought', 'item_domain', 'bought_domain', 'event_info', 'bought_id']].copy()\n",
    "    preds['p'] = p.cpu().numpy()\n",
    "\n",
    "\n",
    "    stack_p.append(preds[['seq_index', 'event_info', 'p']])\n",
    "    preds = preds.sort_values('p', ascending=False).drop_duplicates(subset=['seq_index', 'event_info'])\n",
    "\n",
    "    ytrue = preds.groupby(\"seq_index\")['bought_id'].apply(lambda x: x.iloc[0]).values\n",
    "    ytrue_domain = preds.groupby(\"seq_index\")['bought_domain'].apply(lambda x: x.iloc[0]).values\n",
    "\n",
    "    ypred = preds.groupby(\"seq_index\")['event_info'].apply(lambda x: pad(x.iloc[:10].tolist()))\n",
    "    ypred = np.array(ypred.tolist())\n",
    "\n",
    "    ypred_domain = preds.groupby(\"seq_index\")['item_domain'].apply(lambda x: pad_str(x.iloc[:10].tolist()))\n",
    "    ypred_domain = np.array(ypred_domain.tolist())\n",
    "\n",
    "    print(epoch, ndcg_vec(ytrue, ypred, ytrue_domain, ypred_domain))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8 0.28640934147437763"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked = pd.concat(stack_p)\n",
    "stacked.to_parquet(\"./stack_2f/26_train.parquet\", engine='fastparquet', compression=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked['y'] =  train_melt['y_rank']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked.corr(method='spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.593471 / 0.597317"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr = train_melt\n",
    "Xval = test_melt\n",
    "\n",
    "scaler = StandardScaler()\n",
    "Xtr_nums = scaler.fit_transform(Xtr[features]).astype(np.float32)\n",
    "Xtr_nums[np.isnan(Xtr_nums)] = 0\n",
    "Xval_nums = scaler.transform(Xval[features]).astype(np.float32)\n",
    "Xval_nums[np.isnan(Xval_nums)] = 0\n",
    "\n",
    "torch.manual_seed(0)\n",
    "mdl = NN().cuda()\n",
    "optimizer = Adam(mdl.parameters(), lr=1e-5)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "#del tl, vl\n",
    "t = Dataset(Xtr['seq_index'], Xtr['event_info'], Xtr_nums, Xtr['y_rank'])\n",
    "tl = torch.utils.data.DataLoader(t, batch_size=128, num_workers=8, pin_memory=True, shuffle=True)\n",
    "\n",
    "v = Dataset(Xval['seq_index'], Xval['event_info'], Xval_nums, Xval['event_info']*0)\n",
    "vl = torch.utils.data.DataLoader(v, batch_size=1024, num_workers=4, pin_memory=True, shuffle=False)\n",
    "\n",
    "for epoch in range(5):\n",
    "    tr_loss = 0\n",
    "    tr_cnt = 0\n",
    "    #tl = torch.utils.data.DataLoader(t, batch_size=128, num_workers=8, pin_memory=True, shuffle=True)\n",
    "    for x, c, n, y in tqdm.tqdm(tl):\n",
    "        #print(x.shape, c.shape, y.shape)\n",
    "        x = x.cuda()\n",
    "        c = c.cuda()\n",
    "        n = n.cuda()\n",
    "        y = y.cuda().unsqueeze(-1)\n",
    "\n",
    "        mdl.zero_grad()\n",
    "\n",
    "        p = mdl(x,c,n)\n",
    "\n",
    "        loss = criterion(p, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        tr_cnt += 1\n",
    "        #if tr_cnt % 1000 == 0:\n",
    "        #    print(tr_cnt, tr_loss/tr_cnt)\n",
    "        #    break\n",
    "    print(epoch, tr_loss/tr_cnt)\n",
    "\n",
    "\n",
    "\n",
    "p = list()\n",
    "with torch.no_grad():\n",
    "    for x, c, n, y in tqdm.tqdm(vl):\n",
    "        #print(x.shape, c.shape, y.shape)\n",
    "        x = x.cuda()\n",
    "        c = c.cuda()\n",
    "        n = n.cuda()\n",
    "        p.append(mdl(x,c,n))\n",
    "\n",
    "    p = torch.cat(p)\n",
    "\n",
    "preds = Xval[['seq_index', 'event_info']].copy()\n",
    "preds['p'] = p.cpu().numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.to_parquet(\"./stack_2f/26_test.parquet\", engine='fastparquet', compression=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
